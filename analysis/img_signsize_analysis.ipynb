{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_packages",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow opencv-python-headless matplotlib requests tqdm scikit-learn pandas numpy pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import io\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score,\n",
    "    average_precision_score, precision_recall_curve\n",
    ")\n",
    "from roboflow import Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'roboflow_api_key': \"K0xg5GEEinqPgaqjKKzz\",\n",
    "    'workspace': \"matyworkspace\",\n",
    "    'project': \"damagedhealthytrafficsigns\",\n",
    "    'version': 9,\n",
    "    #'roboflow_api_key': \"LcrgBgwaWa5TXeG1Vtxa\",\n",
    "    #'workspace': \"matyworkspace\",\n",
    "    #'project': \"just-traffic-signs-45axx\",\n",
    "    #'version': 3,\n",
    "    'prediction_url': os.environ.get('PREDICTION_URL', 'http://localhost:3000/predict'),\n",
    "    'class_names': [\"damaged\", \"healthy\"],\n",
    "    'size_groups': 4,\n",
    "    'iou_threshold': 0.5,\n",
    "    'confidence_threshold': 0.4,\n",
    "    'analysis_property': 'size'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Roboflow\n",
    "rf = Roboflow(api_key=CONFIG['roboflow_api_key'])\n",
    "project = rf.workspace(CONFIG['workspace']).project(CONFIG['project'])\n",
    "version = project.version(CONFIG['version'])\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "# Set up paths\n",
    "image_dir = os.path.join(dataset.location, \"test\", \"images\")\n",
    "label_dir = os.path.join(dataset.location, \"test\", \"labels\")\n",
    "original_image_paths = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "\n",
    "print(f\"Loaded {len(original_image_paths)} images from dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sign_size_percentage(bbox: Dict, image_width: int, image_height: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of the full image that a bounding box represents.\n",
    "    \n",
    "    Args:\n",
    "        bbox: Bounding box dictionary with x1, y1, x2, y2 coordinates\n",
    "        image_width: Width of the image in pixels\n",
    "        image_height: Height of the image in pixels\n",
    "    \n",
    "    Returns:\n",
    "        Percentage of the image area occupied by the bounding box\n",
    "    \"\"\"\n",
    "    bbox_width = bbox[\"x2\"] - bbox[\"x1\"]\n",
    "    bbox_height = bbox[\"y2\"] - bbox[\"y1\"]\n",
    "    bbox_area = bbox_width * bbox_height\n",
    "    image_area = image_width * image_height\n",
    "    \n",
    "    size_percentage = (bbox_area / image_area) * 100\n",
    "    return size_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_sizes(image_paths: List[str], label_dir: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Analyze the sizes of traffic signs in the dataset using ground truth data only.\n",
    "    \"\"\"\n",
    "    size_data = []\n",
    "    \n",
    "    print(f\"Analyzing sizes for {len(image_paths)} images...\")\n",
    "    \n",
    "    for image_path in tqdm(image_paths, desc=\"Analyzing image sizes\"):\n",
    "        # Load image to get dimensions\n",
    "        image = cv2.imread(image_path)\n",
    "        h_img, w_img = image.shape[:2]\n",
    "        \n",
    "        # Load ground truth bounding boxes\n",
    "        filename = os.path.basename(image_path).replace('.jpg', '.txt')\n",
    "        label_path = os.path.join(label_dir, filename)\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    class_id, cx, cy, w, h = map(float, line.strip().split())\n",
    "                    x_center = cx * w_img\n",
    "                    y_center = cy * h_img\n",
    "                    width = w * w_img\n",
    "                    height = h * h_img\n",
    "                    x1 = int(x_center - width / 2)\n",
    "                    y1 = int(y_center - height / 2)\n",
    "                    x2 = int(x_center + width / 2)\n",
    "                    y2 = int(y_center + height / 2)\n",
    "                    \n",
    "                    bbox = {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2}\n",
    "                    size_percentage = calculate_sign_size_percentage(bbox, w_img, h_img)\n",
    "                    \n",
    "                    size_data.append({\n",
    "                        \"image\": image_path,\n",
    "                        \"class\": CONFIG['class_names'][int(class_id)],\n",
    "                        \"size_percentage\": size_percentage,\n",
    "                        \"image_width\": w_img,\n",
    "                        \"image_height\": h_img\n",
    "                    })\n",
    "    \n",
    "    return size_data\n",
    "\n",
    "def plot_dataset_size_histogram_with_groups(size_data: List[Dict], num_groups: int = 4, \n",
    "                                           save_path: str = None, figsize=(8, 4)):\n",
    "    \"\"\"\n",
    "    Plot histogram of sign sizes from dataset with group boundaries marked.\n",
    "    \"\"\"\n",
    "    if not size_data:\n",
    "        print(\"No size data available to plot\")\n",
    "        return\n",
    "    \n",
    "    # Extract size percentages and create groups\n",
    "    sizes = [d['size_percentage'] for d in size_data]\n",
    "    sizes_sorted = sorted(sizes)\n",
    "    \n",
    "    # Calculate group boundaries using quantiles\n",
    "    group_size = len(sizes_sorted) // num_groups\n",
    "    boundaries = []\n",
    "    \n",
    "    for i in range(num_groups + 1):\n",
    "        if i == 0:\n",
    "            boundaries.append(min(sizes_sorted))\n",
    "        elif i == num_groups:\n",
    "            boundaries.append(max(sizes_sorted))\n",
    "        else:\n",
    "            idx = i * group_size\n",
    "            if idx < len(sizes_sorted):\n",
    "                boundaries.append(sizes_sorted[idx])\n",
    "            else:\n",
    "                boundaries.append(max(sizes_sorted))\n",
    "    \n",
    "    # Count items in each group for labeling\n",
    "    group_counts = []\n",
    "    for i in range(num_groups):\n",
    "        min_size = boundaries[i]\n",
    "        max_size = boundaries[i+1]\n",
    "        \n",
    "        if i == 0:  # First group, include minimum\n",
    "            count = sum(1 for s in sizes if min_size <= s <= max_size)\n",
    "        else:  # Other groups, exclude minimum to avoid overlap\n",
    "            count = sum(1 for s in sizes if min_size < s <= max_size)\n",
    "        \n",
    "        group_counts.append(count)\n",
    "    \n",
    "    # Create the histogram\n",
    "    fig, (ax2) = plt.subplots(1, 1, figsize=figsize)\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'brown']\n",
    "    \n",
    "    group_data_for_box = []\n",
    "    group_labels = []\n",
    "    \n",
    "    for i in range(num_groups):\n",
    "        min_size = boundaries[i]\n",
    "        max_size = boundaries[i+1]\n",
    "        \n",
    "        if i == 0:  # First group, include minimum\n",
    "            group_sizes = [s for s in sizes if min_size <= s <= max_size]\n",
    "        else:  # Other groups, exclude minimum to avoid overlap\n",
    "            group_sizes = [s for s in sizes if min_size < s <= max_size]\n",
    "        \n",
    "        group_data_for_box.append(group_sizes)\n",
    "        group_labels.append(f\"Group {i+1}\\n{min_size:.2f}% - {max_size:.2f}%\")\n",
    "    \n",
    "    box_plot = ax2.boxplot(group_data_for_box, labels=group_labels, patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    for i, patch in enumerate(box_plot['boxes']):\n",
    "        patch.set_facecolor(colors[i % len(colors)])\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Size Groups', fontweight='bold')\n",
    "    ax2.set_ylabel('Sign Size (% of image area)', fontweight='bold')\n",
    "    ax2.set_title('Size Distribution by Groups', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved dataset size histogram to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDataset Size Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total signs: {len(sizes)}\")\n",
    "    print(f\"Min size: {min(sizes):.2f}%\")\n",
    "    print(f\"Max size: {max(sizes):.2f}%\") \n",
    "    print(f\"Mean size: {np.mean(sizes):.2f}%\")\n",
    "    print(f\"Median size: {np.median(sizes):.2f}%\")\n",
    "    print(f\"Std deviation: {np.std(sizes):.2f}%\")\n",
    "    \n",
    "    for i in range(num_groups):\n",
    "        print(f\"Group {i+1}: {boundaries[i]:.2f}% - {boundaries[i+1]:.2f}% ({group_counts[i]} signs)\")\n",
    "\n",
    "# Analyze dataset sizes and plot histogram\n",
    "print(\"Analyzing dataset sign sizes...\")\n",
    "dataset_size_data = analyze_dataset_sizes(original_image_paths, label_dir)\n",
    "\n",
    "# Plot dataset size distribution\n",
    "dataset_plots_dir = os.path.join(dataset.location, \"dataset_analysis\")\n",
    "os.makedirs(dataset_plots_dir, exist_ok=True)\n",
    "dataset_histogram_path = os.path.join(dataset_plots_dir, 'dataset_size_histogram.png')\n",
    "\n",
    "plot_dataset_size_histogram_with_groups(dataset_size_data, CONFIG['size_groups'], dataset_histogram_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_image_for_prediction(image_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Send image to prediction API and return results.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_data = f.read()\n",
    "        files = {\n",
    "            'image': ('image.jpg', io.BytesIO(image_data), 'image/jpeg')\n",
    "        }\n",
    "        response = requests.post(\n",
    "            CONFIG['prediction_url'],\n",
    "            files=files,\n",
    "            timeout=30\n",
    "        )\n",
    "        return response.json()\n",
    "\n",
    "def compute_iou(boxA: Dict, boxB: Dict) -> float:\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union (IoU) of two bounding boxes.\n",
    "    \"\"\"\n",
    "    xA = max(boxA[\"x1\"], boxB[\"x1\"])\n",
    "    yA = max(boxA[\"y1\"], boxB[\"y1\"])\n",
    "    xB = min(boxA[\"x2\"], boxB[\"x2\"])\n",
    "    yB = min(boxA[\"y2\"], boxB[\"y2\"])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[\"x2\"] - boxA[\"x1\"]) * (boxA[\"y2\"] - boxA[\"y1\"])\n",
    "    boxBArea = (boxB[\"x2\"] - boxB[\"x1\"]) * (boxB[\"y2\"] - boxB[\"y1\"])\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_brightness_groups",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions_with_sizes(image_paths: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate predictions for original images and calculate sign sizes.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Processing predictions for {len(image_paths)} images...\")\n",
    "    \n",
    "    for original_path in tqdm(image_paths, desc=\"Processing images\"):\n",
    "        image = cv2.imread(original_path)\n",
    "        h_img, w_img = image.shape[:2]\n",
    "        \n",
    "        filename = os.path.basename(original_path).replace('.jpg', '.txt')\n",
    "        label_path = os.path.join(label_dir, filename)\n",
    "        \n",
    "        gt_bboxes = []\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    class_id, cx, cy, w, h = map(float, line.strip().split())\n",
    "                    x_center = cx * w_img\n",
    "                    y_center = cy * h_img\n",
    "                    width = w * w_img\n",
    "                    height = h * h_img\n",
    "                    x1 = int(x_center - width / 2)\n",
    "                    y1 = int(y_center - height / 2)\n",
    "                    x2 = int(x_center + width / 2)\n",
    "                    y2 = int(y_center + height / 2)\n",
    "                    gt_bboxes.append({\n",
    "                        \"class\": CONFIG['class_names'][int(class_id)],\n",
    "                        \"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2\n",
    "                    })\n",
    "        \n",
    "        try:\n",
    "            predictions = send_image_for_prediction(original_path)\n",
    "            pred_bboxes = [{**p['box'], **p} for p in predictions]\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting predictions for {original_path}: {e}\")\n",
    "            pred_bboxes = []\n",
    "        \n",
    "        gt_bboxes_matched = [False] * len(gt_bboxes)\n",
    "        \n",
    "        for pred in pred_bboxes:\n",
    "            class_score = pred['cls_score']\n",
    "            healthy_score = pred['confidence'] * class_score\n",
    "            damaged_score = pred['confidence'] * (1 - class_score)\n",
    "\n",
    "            if healthy_score < CONFIG['confidence_threshold'] and damaged_score < CONFIG['confidence_threshold']:\n",
    "                predicted_label = \"background\"\n",
    "            elif healthy_score > damaged_score:\n",
    "                predicted_label = \"healthy\"\n",
    "            else:\n",
    "                predicted_label = \"damaged\"\n",
    "            \n",
    "            best_iou = 0\n",
    "            best_gt_idx = None\n",
    "            actual_label = \"unknown\"\n",
    "            \n",
    "            for i, gt in enumerate(gt_bboxes):\n",
    "                iou = compute_iou(pred, gt)\n",
    "                if iou >= CONFIG['iou_threshold'] and iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = i\n",
    "                    actual_label = gt[\"class\"]\n",
    "            \n",
    "            if best_gt_idx is not None:\n",
    "                gt_bboxes_matched[best_gt_idx] = True\n",
    "                size_percentage = calculate_sign_size_percentage(gt_bboxes[best_gt_idx], w_img, h_img)\n",
    "            else:\n",
    "                size_percentage = None  # No ground truth match, will be filtered out\n",
    "            \n",
    "            if best_gt_idx is not None:\n",
    "                results.append({\n",
    "                    \"image\": original_path,\n",
    "                    \"actual_bbox\": gt_bboxes[best_gt_idx],\n",
    "                    \"predicted_bbox\": {\n",
    "                        \"x1\": int(pred[\"x1\"]),\n",
    "                        \"y1\": int(pred[\"y1\"]),\n",
    "                        \"x2\": int(pred[\"x2\"]),\n",
    "                        \"y2\": int(pred[\"y2\"])\n",
    "                    },\n",
    "                    \"actual\": actual_label,\n",
    "                    \"predicted\": predicted_label,\n",
    "                    \"class_score\": class_score,\n",
    "                    \"confidence\": pred[\"confidence\"],\n",
    "                    \"iou\": best_iou,\n",
    "                    \"size_percentage\": size_percentage,\n",
    "                    \"image_width\": w_img,\n",
    "                    \"image_height\": h_img\n",
    "                })\n",
    "        \n",
    "        # Add unmatched ground truth bboxes as false negatives (missed detections)\n",
    "        for i, matched in enumerate(gt_bboxes_matched):\n",
    "            if not matched:\n",
    "                size_percentage = calculate_sign_size_percentage(gt_bboxes[i], w_img, h_img)\n",
    "                results.append({\n",
    "                    \"image\": original_path,\n",
    "                    \"actual_bbox\": gt_bboxes[i],\n",
    "                    \"predicted_bbox\": {},\n",
    "                    \"actual\": gt_bboxes[i][\"class\"],\n",
    "                    \"predicted\": \"none\",  # Missed detection\n",
    "                    \"class_score\": 0,\n",
    "                    \"confidence\": 0,\n",
    "                    \"iou\": 0,\n",
    "                    \"size_percentage\": size_percentage,\n",
    "                    \"image_width\": w_img,\n",
    "                    \"image_height\": h_img\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Processing original images for size analysis...\")\n",
    "all_results = process_predictions_with_sizes(original_image_paths)\n",
    "\n",
    "print(f\"Generated {len(all_results)} prediction results with size information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_size_groups(results: List[Dict], num_groups: int = 4) -> Dict:\n",
    "    \"\"\"\n",
    "    Create size groups with equal number of predictions in each group.\n",
    "    \n",
    "    Args:\n",
    "        results: List of prediction results with size_percentage\n",
    "        num_groups: Number of size groups to create\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with size groups and their boundaries\n",
    "    \"\"\"\n",
    "    sizes = [r['size_percentage'] for r in results]\n",
    "    sizes_sorted = sorted(sizes)\n",
    "    \n",
    "    group_size = len(sizes_sorted) // num_groups\n",
    "    boundaries = []\n",
    "    \n",
    "    for i in range(num_groups + 1):\n",
    "        if i == 0:\n",
    "            boundaries.append(min(sizes_sorted))\n",
    "        elif i == num_groups:\n",
    "            boundaries.append(max(sizes_sorted))\n",
    "        else:\n",
    "            idx = i * group_size\n",
    "            if idx < len(sizes_sorted):\n",
    "                boundaries.append(sizes_sorted[idx])\n",
    "            else:\n",
    "                boundaries.append(max(sizes_sorted))\n",
    "    \n",
    "    size_groups = {}\n",
    "    group_stats = []\n",
    "    \n",
    "    for i in range(num_groups):\n",
    "        group_name = f\"size_group_{i+1}\"\n",
    "        min_size = boundaries[i]\n",
    "        max_size = boundaries[i+1]\n",
    "        \n",
    "        group_results = []\n",
    "        for result in results:\n",
    "            size = result['size_percentage']\n",
    "            if i == 0:  # First group, include minimum\n",
    "                if min_size <= size <= max_size:\n",
    "                    group_results.append(result)\n",
    "            else:  # Other groups, exclude minimum to avoid overlap\n",
    "                if min_size < size <= max_size:\n",
    "                    group_results.append(result)\n",
    "        \n",
    "        size_groups[group_name] = {\n",
    "            'results': group_results,\n",
    "            'min_size': min_size,\n",
    "            'max_size': max_size,\n",
    "            'size_range': f\"{min_size:.2f}% - {max_size:.2f}%\",\n",
    "            'count': len(group_results)\n",
    "        }\n",
    "        \n",
    "        group_stats.append({\n",
    "            'group': group_name,\n",
    "            'min_size': min_size,\n",
    "            'max_size': max_size,\n",
    "            'count': len(group_results)\n",
    "        })\n",
    "    \n",
    "    # Print group statistics\n",
    "    print(\"\\nSize Group Statistics:\")\n",
    "    print(\"=\" * 60)\n",
    "    for stat in group_stats:\n",
    "        print(f\"{stat['group']:15} | Range: {stat['min_size']:6.2f}% - {stat['max_size']:6.2f}% | Count: {stat['count']:4d}\")\n",
    "    \n",
    "    return size_groups\n",
    "\n",
    "print(\"Creating size groups...\")\n",
    "size_groups = create_size_groups(all_results, CONFIG['size_groups'])\n",
    "\n",
    "base_dir = os.path.join(dataset.location, f\"{CONFIG['analysis_property']}_analysis\")\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "predictions_file = os.path.join(base_dir, \"predictions_results.json\")\n",
    "with open(predictions_file, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "size_groups_file = os.path.join(base_dir, \"size_groups.json\")\n",
    "size_groups_serializable = {}\n",
    "for group_name, group_data in size_groups.items():\n",
    "    size_groups_serializable[group_name] = {\n",
    "        'min_size': group_data['min_size'],\n",
    "        'max_size': group_data['max_size'],\n",
    "        'size_range': group_data['size_range'],\n",
    "        'count': group_data['count']\n",
    "    }\n",
    "\n",
    "with open(size_groups_file, \"w\") as f:\n",
    "    json.dump(size_groups_serializable, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved {len(all_results)} prediction results to {predictions_file}\")\n",
    "print(f\"Saved size group information to {size_groups_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics_calculation",
   "metadata": {},
   "source": [
    "## Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d22aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall_metrics(group_results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate recall metrics for a size group.\n",
    "    Focus on recall since we're interested in detection performance.\n",
    "    \"\"\"\n",
    "    if not group_results:\n",
    "        return {\n",
    "            'recall_healthy': 0.0,\n",
    "            'recall_damaged': 0.0,\n",
    "            'recall_overall': 0.0,\n",
    "            'total_healthy': 0,\n",
    "            'total_damaged': 0,\n",
    "            'detected_healthy': 0,\n",
    "            'detected_damaged': 0,\n",
    "            'sample_count': 0\n",
    "        }\n",
    "    \n",
    "    total_healthy = sum(1 for r in group_results if r['actual'] == 'healthy')\n",
    "    total_damaged = sum(1 for r in group_results if r['actual'] == 'damaged')\n",
    "    \n",
    "    detected_healthy = sum(1 for r in group_results \n",
    "                          if r['actual'] == 'healthy' and r['predicted'] == 'healthy')\n",
    "    detected_damaged = sum(1 for r in group_results \n",
    "                          if r['actual'] == 'damaged' and r['predicted'] == 'damaged')\n",
    "    \n",
    "    undetected_healthy = sum(1 for r in group_results\n",
    "                             if r['actual'] == 'healthy' and r['predicted'] == 'none')\n",
    "    undetected_damaged = sum(1 for r in group_results\n",
    "                                if r['actual'] == 'damaged' and r['predicted'] == 'none')\n",
    "    undetected_background_healthy = sum(1 for r in group_results\n",
    "                                  if r['actual'] == 'healthy' and r['predicted'] == 'background')\n",
    "    undetected_background_damaged = sum(1 for r in group_results\n",
    "                                  if r['actual'] == 'damaged' and r['predicted'] == 'background')\n",
    "    \n",
    "    recall_healthy = detected_healthy / total_healthy if total_healthy > 0 else 0.0\n",
    "    recall_damaged = detected_damaged / total_damaged if total_damaged > 0 else 0.0\n",
    "    \n",
    "    recall_overall = (recall_healthy + recall_damaged) / 2.0 if (total_healthy > 0 or total_damaged > 0) else 0.0\n",
    "    \n",
    "    return {\n",
    "        'recall_healthy': float(recall_healthy),\n",
    "        'recall_damaged': float(recall_damaged),\n",
    "        'recall_overall': float(recall_overall),\n",
    "        'total_healthy': int(total_healthy),\n",
    "        'total_damaged': int(total_damaged),\n",
    "        'detected_healthy': int(detected_healthy),\n",
    "        'detected_damaged': int(detected_damaged),\n",
    "        'sample_count': len(group_results),\n",
    "        'undetected_healthy': int(undetected_healthy),\n",
    "        'undetected_damaged': int(undetected_damaged),\n",
    "        'undetected_background_healthy': int(undetected_background_healthy),\n",
    "        'undetected_background_damaged': int(undetected_background_damaged)\n",
    "    }\n",
    "\n",
    "def calculate_size_group_metrics(size_groups: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate recall metrics for all size groups.\n",
    "    \"\"\"\n",
    "    group_metrics = {}\n",
    "    \n",
    "    print(\"\\nCalculating metrics for size groups:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for group_name, group_data in size_groups.items():\n",
    "        metrics = calculate_recall_metrics(group_data['results'])\n",
    "        \n",
    "        metrics.update({\n",
    "            'min_size': group_data['min_size'],\n",
    "            'max_size': group_data['max_size'],\n",
    "            'size_range': group_data['size_range'],\n",
    "            'avg_size': np.mean([r['size_percentage'] for r in group_data['results']])\n",
    "        })\n",
    "        \n",
    "        group_metrics[group_name] = metrics\n",
    "        \n",
    "        print(f\"{group_name:15} | Range: {metrics['size_range']:15} | \"\n",
    "              f\"Samples: {metrics['sample_count']:3d} | \"\n",
    "              f\"Recall H: {metrics['recall_healthy']:.3f} | \"\n",
    "              f\"Recall D: {metrics['recall_damaged']:.3f} | \"\n",
    "              f\"Overall: {metrics['recall_overall']:.3f}\")\n",
    "    \n",
    "    return group_metrics\n",
    "\n",
    "group_metrics = calculate_size_group_metrics(size_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recall_by_size_groups(group_metrics: Dict, save_path: str = None, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    Plot recall metrics as bar plots for each size group.\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "    recall_healthy = []\n",
    "    recall_damaged = []\n",
    "    recall_overall = []\n",
    "    size_ranges = []\n",
    "    total_healthy_counts = []\n",
    "    total_damaged_counts = []\n",
    "    \n",
    "    sorted_groups = sorted(group_metrics.items(), key=lambda x: x[1]['avg_size'])\n",
    "    \n",
    "    for group_name, metrics in sorted_groups:\n",
    "        group_names.append(group_name.replace('size_group_', 'Group '))\n",
    "        recall_healthy.append(metrics['recall_healthy'])\n",
    "        recall_damaged.append(metrics['recall_damaged'])\n",
    "        recall_overall.append(metrics['recall_overall'])\n",
    "        size_ranges.append(metrics['size_range'])\n",
    "        total_healthy_counts.append(metrics['total_healthy'])\n",
    "        total_damaged_counts.append(metrics['total_damaged'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    x = np.arange(len(group_names))\n",
    "    width = 0.6\n",
    "    \n",
    "    # Plot 1: Group composition (count of healthy vs damaged)\n",
    "    axes[0].bar(x, total_damaged_counts, width, label='Damaged', alpha=0.8, color='red')\n",
    "    axes[0].bar(x, total_healthy_counts, width, label='Healthy', alpha=0.8, color='green', bottom=total_damaged_counts)\n",
    "    \n",
    "    axes[0].set_xlabel('Size Group', fontweight='bold')\n",
    "    axes[0].set_ylabel('Traffic Sign Count', fontweight='bold')\n",
    "    axes[0].set_title('a) Group Composition by Class', fontweight='bold')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(group_names, rotation=45)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, (h, d) in enumerate(zip(total_healthy_counts, total_damaged_counts)):\n",
    "        if d > 0:  \n",
    "            axes[0].text(i, d/2, f'{d}', ha='center', va='center', fontweight='bold', color='white')\n",
    "        if h > 0:  \n",
    "            axes[0].text(i, d + h/2, f'{h}', ha='center', va='center', fontweight='bold', color='white')\n",
    "    \n",
    "    # Plot 2: Individual class recalls\n",
    "    x_offset = np.arange(len(group_names))\n",
    "    axes[1].bar(x_offset - width/4, recall_healthy, width/2, label='Healthy', alpha=0.8, color='green')\n",
    "    axes[1].bar(x_offset + width/4, recall_damaged, width/2, label='Damaged', alpha=0.8, color='red')\n",
    "    \n",
    "    axes[1].set_xlabel('Size Group', fontweight='bold')\n",
    "    axes[1].set_ylabel('Recall Score', fontweight='bold')\n",
    "    axes[1].set_title('b) Recall by Class', fontweight='bold')\n",
    "    axes[1].set_xticks(x_offset)\n",
    "    axes[1].set_xticklabels(group_names, rotation=45)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_ylim(0, 1.0)\n",
    "    \n",
    "    for i, h in enumerate(recall_healthy):\n",
    "        if h > 0.05:\n",
    "            axes[1].text(i - width/4, h + 0.02, f'{h:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    for i, d in enumerate(recall_damaged):\n",
    "        if d > 0.05:\n",
    "            axes[1].text(i + width/4, d + 0.02, f'{d:.2f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Overall recall\n",
    "    bars = axes[2].bar(x, recall_overall, width, alpha=0.8, color='blue')\n",
    "    \n",
    "    axes[2].set_xlabel('Size Group', fontweight='bold')\n",
    "    axes[2].set_ylabel('Overall Recall Score', fontweight='bold')\n",
    "    axes[2].set_title('c) Overall Recall by Size Group', fontweight='bold')\n",
    "    axes[2].set_xticks(x)\n",
    "    axes[2].set_xticklabels(group_names, rotation=45)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].set_ylim(0, 1.0)\n",
    "    \n",
    "    for i, (bar, recall) in enumerate(zip(bars, recall_overall)):\n",
    "        if recall > 0.05:\n",
    "            axes[2].text(i, recall + 0.02, f'{recall:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved recall plots to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nGenerating plots...\")\n",
    "\n",
    "plots_dir = os.path.join(base_dir, 'plots')\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "recall_plot_path = os.path.join(plots_dir, 'recall_by_size_groups.png')\n",
    "plot_recall_by_size_groups(group_metrics, recall_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detection_outcomes_by_group_and_class(group_metrics, size_groups, save_path=None, figsize=(14, 7)):\n",
    "    \n",
    "    group_names = []\n",
    "    class_labels = ['healthy', 'damaged']\n",
    "    outcomes = ['correct', 'incorrect', 'background', 'none']\n",
    "    outcome_colors = {\n",
    "        'correct': \"#10C716\",      \n",
    "        'incorrect': \"#FFEE00\",    \n",
    "        'background': \"#FF7B00\",   \n",
    "        'none': \"#AA1C12\"         \n",
    "    }\n",
    "    outcomes_labels = {\n",
    "        'correct': 'Detected and Classified Correctly',\n",
    "        'incorrect': 'Detected but Misclassified',\n",
    "        'background': 'Detected as Background',\n",
    "        'none': 'Undetected'\n",
    "    }\n",
    "\n",
    "    proportions = {cls: {out: [] for out in outcomes} for cls in class_labels}\n",
    "\n",
    "    sorted_groups = sorted(group_metrics.items(), key=lambda x: x[1]['avg_size'])\n",
    "    for group_name, metrics in sorted_groups:\n",
    "        group_names.append(group_name.replace('size_group_', 'Group '))\n",
    "        group_results = size_groups[group_name]['results']\n",
    "\n",
    "        for cls in class_labels:\n",
    "            total = sum(1 for r in group_results if r['actual'] == cls)\n",
    "            if total == 0:\n",
    "                for out in outcomes:\n",
    "                    proportions[cls][out].append(0)\n",
    "                continue\n",
    "\n",
    "            # Correct: predicted == actual\n",
    "            correct = sum(1 for r in group_results if r['actual'] == cls and r['predicted'] == cls)\n",
    "            # Incorrect: predicted != actual and predicted in class_labels\n",
    "            incorrect = sum(1 for r in group_results if r['actual'] == cls and r['predicted'] in class_labels and r['predicted'] != cls)\n",
    "            # Background: predicted == 'background'\n",
    "            background = sum(1 for r in group_results if r['actual'] == cls and r['predicted'] == 'background')\n",
    "            # None: predicted == 'none'\n",
    "            none = sum(1 for r in group_results if r['actual'] == cls and r['predicted'] == 'none')\n",
    "\n",
    "            proportions[cls]['correct'].append(correct / total)\n",
    "            proportions[cls]['incorrect'].append(incorrect / total)\n",
    "            proportions[cls]['background'].append(background / total)\n",
    "            proportions[cls]['none'].append(none / total)\n",
    "\n",
    "    title_fs = 18\n",
    "    label_fs = 16\n",
    "    tick_fs = 14\n",
    "    legend_fs = 13\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize, sharey=False)\n",
    "    width = 0.6\n",
    "    x = np.arange(len(group_names))\n",
    "\n",
    "    for idx, cls in enumerate(class_labels):\n",
    "        bottom = np.zeros(len(group_names))\n",
    "        for out in outcomes:\n",
    "            axes[idx].bar(x, proportions[cls][out], width, bottom=bottom, label=outcomes_labels[out], color=outcome_colors[out], alpha=0.85)\n",
    "            bottom += np.array(proportions[cls][out])\n",
    "        axes[idx].set_title(f\"Detection Outcomes for {cls.capitalize()} Signs\", fontweight='bold', fontsize=title_fs)\n",
    "        axes[idx].set_xticks(x)\n",
    "        axes[idx].set_xticklabels(group_names, fontsize=tick_fs)\n",
    "        axes[idx].set_ylim(0, 1.0)\n",
    "        axes[idx].set_ylabel(\"Proportion of Detections\", fontsize=label_fs)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        axes[idx].legend(loc='lower right', fontsize=legend_fs)\n",
    "        axes[idx].tick_params(axis='y', labelsize=tick_fs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved detection outcomes plot to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "outcomes_plot_path = os.path.join(plots_dir, 'detection_outcomes_by_group_and_class.png')\n",
    "plot_detection_outcomes_by_group_and_class(group_metrics, size_groups, outcomes_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e22ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_size_analysis_results(group_metrics: Dict, all_results: List[Dict], output_dir: str):\n",
    "    metrics_json_path = os.path.join(output_dir, 'size_group_metrics.json')\n",
    "    with open(metrics_json_path, 'w') as f:\n",
    "        json.dump(group_metrics, f, indent=2)\n",
    "    print(f\"Saved group metrics to {metrics_json_path}\")\n",
    "    \n",
    "    summary_stats = {\n",
    "        'total_predictions': len(all_results),\n",
    "        'size_statistics': {\n",
    "            'min_size': min(r['size_percentage'] for r in all_results),\n",
    "            'max_size': max(r['size_percentage'] for r in all_results),\n",
    "            'mean_size': np.mean([r['size_percentage'] for r in all_results]),\n",
    "            'median_size': np.median([r['size_percentage'] for r in all_results]),\n",
    "            'std_size': np.std([r['size_percentage'] for r in all_results])\n",
    "        },\n",
    "        'class_distribution': {\n",
    "            'healthy': sum(1 for r in all_results if r['actual'] == 'healthy'),\n",
    "            'damaged': sum(1 for r in all_results if r['actual'] == 'damaged')\n",
    "        },\n",
    "        'overall_recall': {\n",
    "            'healthy': sum(1 for r in all_results if r['actual'] == 'healthy' and r['predicted'] == 'healthy') / \n",
    "                      sum(1 for r in all_results if r['actual'] == 'healthy'),\n",
    "            'damaged': sum(1 for r in all_results if r['actual'] == 'damaged' and r['predicted'] == 'damaged') / \n",
    "                      sum(1 for r in all_results if r['actual'] == 'damaged')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_json_path = os.path.join(output_dir, 'analysis_summary.json')\n",
    "    with open(summary_json_path, 'w') as f:\n",
    "        json.dump(summary_stats, f, indent=2)\n",
    "    print(f\"Saved analysis summary to {summary_json_path}\")\n",
    "\n",
    "save_size_analysis_results(group_metrics, all_results, base_dir)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIZE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions analyzed: {len(all_results)}\")\n",
    "print(f\"Number of size groups: {CONFIG['size_groups']}\")\n",
    "print(f\"Results saved to: {base_dir}\")\n",
    "print(\"\\nOverall Recall Performance:\")\n",
    "for group_name, metrics in group_metrics.items():\n",
    "    print(f\"{group_name:15} | {metrics['size_range']:15} | \"\n",
    "          f\"Overall Recall: {metrics['recall_overall']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

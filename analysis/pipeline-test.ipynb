{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a09d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow opencv-python-headless matplotlib requests tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc951a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"K0xg5GEEinqPgaqjKKzz\")\n",
    "project = rf.workspace(\"matyworkspace\").project(\"damagedhealthytrafficsigns\")\n",
    "version = project.version(9)\n",
    "dataset = version.download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f450865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "# Load image paths\n",
    "image_dir = os.path.join(dataset.location, \"test\", \"images\")\n",
    "label_dir = os.path.join(dataset.location, \"test\", \"labels\")\n",
    "image_paths = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "\n",
    "# Load class names\n",
    "class_names = [\"damaged\", \"healthy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import os\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8165516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_random_images_with_bboxes(image_paths, label_dir, class_names, N=9, cols=None):\n",
    "    sample_paths = random.sample(image_paths, min(N, len(image_paths)))\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "\n",
    "    for img_path in sample_paths:\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        bbox_list = []\n",
    "        label_list = []\n",
    "        label_path = os.path.join(label_dir, os.path.basename(img_path).replace('.jpg', '.txt'))\n",
    "        if os.path.exists(label_path):\n",
    "            h_img, w_img = img.shape[:2]\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    class_id, cx, cy, w, h = map(float, line.strip().split())\n",
    "                    x_center = cx * w_img\n",
    "                    y_center = cy * h_img\n",
    "                    width = w * w_img\n",
    "                    height = h * h_img\n",
    "                    x1 = int(x_center - width / 2)\n",
    "                    y1 = int(y_center - height / 2)\n",
    "                    x2 = int(x_center + width / 2)\n",
    "                    y2 = int(y_center + height / 2)\n",
    "                    bbox_list.append((x1, y1, x2, y2))\n",
    "                    label_list.append(class_names[int(class_id)])\n",
    "        images.append(img)\n",
    "        bboxes.append(bbox_list)\n",
    "        labels.append(label_list)\n",
    "\n",
    "    cols = int(np.ceil(np.sqrt(N)) if cols is None else cols)\n",
    "    rows = int(np.ceil(N / cols))\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (img, bbox_list, label_list) in enumerate(zip(images, bboxes, labels)):\n",
    "        img_draw = img.copy()\n",
    "        h_img, w_img = img.shape[:2]\n",
    "        thickness = max(2, int(round(0.005 * (h_img + w_img) / 2)))\n",
    "        font_scale = max(0.5, 0.002 * (h_img + w_img) / 2)\n",
    "        for bbox, label in zip(bbox_list, label_list):\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            color = (0, 255, 0) if label == \"healthy\" else (255, 0, 0)\n",
    "            cv2.rectangle(img_draw, (x1, y1), (x2, y2), color, thickness)\n",
    "            cv2.putText(img_draw, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)\n",
    "        axes[i].imshow(img_draw)\n",
    "        axes[i].axis('off')\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_random_images_with_bboxes(image_paths, label_dir, class_names, N=8, cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623dde24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import os\n",
    "\n",
    "def send_image_for_prediction(image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_data = f.read()\n",
    "        files = {\n",
    "            'image': ('image.jpg', io.BytesIO(image_data), 'image/jpeg')\n",
    "        }\n",
    "        prediction_url = os.environ.get('PREDICTION_URL', 'http://localhost:3000/predict')\n",
    "        response = requests.post(\n",
    "            prediction_url,\n",
    "            files=files,\n",
    "            timeout=30\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503605a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_bboxes(img, bboxes, color=(0, 255, 0), labels=None, confidences=None):\n",
    "    img = img.copy()\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = int(bbox[\"x1\"]), int(bbox[\"y1\"]), int(bbox[\"x2\"]), int(bbox[\"y2\"])\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "        label = \"\"\n",
    "        if labels:\n",
    "            label = labels[i]\n",
    "        if confidences:\n",
    "            label += f\" {confidences[i]:.2f}\"\n",
    "        if label:\n",
    "            cv2.putText(img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    return img\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    xA = max(boxA[\"x1\"], boxB[\"x1\"])\n",
    "    yA = max(boxA[\"y1\"], boxB[\"y1\"])\n",
    "    xB = min(boxA[\"x2\"], boxB[\"x2\"])\n",
    "    yB = min(boxA[\"y2\"], boxB[\"y2\"])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[\"x2\"] - boxA[\"x1\"]) * (boxA[\"y2\"] - boxA[\"y1\"])\n",
    "    boxBArea = (boxB[\"x2\"] - boxB[\"x1\"]) * (boxB[\"y2\"] - boxB[\"y1\"])\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "IOU_THRESHOLD = 0.5\n",
    "\n",
    "results = []\n",
    "print(f\"Processing {len(image_paths)} images...\")\n",
    "\n",
    "for image_path in tqdm(image_paths):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    filename = os.path.basename(image_path).replace('.jpg', '.txt')\n",
    "    label_path = os.path.join(label_dir, filename)\n",
    "\n",
    "    # Load ground truth bounding boxes\n",
    "    gt_bboxes = []\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            class_id, cx, cy, w, h = map(float, line.strip().split())\n",
    "            h_img, w_img = image.shape[:2]\n",
    "            x_center = cx * w_img\n",
    "            y_center = cy * h_img\n",
    "            width = w * w_img\n",
    "            height = h * h_img\n",
    "            x1 = int(x_center - width / 2)\n",
    "            y1 = int(y_center - height / 2)\n",
    "            x2 = int(x_center + width / 2)\n",
    "            y2 = int(y_center + height / 2)\n",
    "            gt_bboxes.append({\n",
    "                \"class\": class_names[int(class_id)],\n",
    "                \"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2\n",
    "            })\n",
    "\n",
    "    predictions = send_image_for_prediction(image_path).json()\n",
    "    predictions = sorted(predictions, key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "    pred_bboxes = [{**p['box'], **p} for p in predictions]\n",
    "\n",
    "\n",
    "    gt_bboxes_predicted = [False for _ in gt_bboxes]\n",
    "    for p in pred_bboxes:\n",
    "        class_id = p['cls_score']\n",
    "\n",
    "        # Find the closest ground truth bbox for label\n",
    "        best_iou = 0\n",
    "        actual_label = \"unknown\"\n",
    "        gt_idx = None\n",
    "        for i, gt in enumerate(gt_bboxes):\n",
    "            if gt_bboxes_predicted[i]:\n",
    "                continue\n",
    "            iou = compute_iou(p, gt)\n",
    "            if iou >= IOU_THRESHOLD and iou > best_iou:\n",
    "                best_iou = iou\n",
    "                actual_label = gt[\"class\"]\n",
    "                gt_idx = i\n",
    "        if gt_idx is not None:\n",
    "            gt_bboxes_predicted[gt_idx] = True\n",
    "\n",
    "        # Show cropped image and label\n",
    "        #plt.imshow(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
    "        #plt.title(f\"Actual: {actual_label} | Predicted: {predicted_label}\")\n",
    "        #plt.axis(\"off\")\n",
    "        #plt.show()\n",
    "\n",
    "        # Save result\n",
    "        results.append({\n",
    "            \"image\": image_path,\n",
    "            \"actual_bbox\": {\n",
    "                \"x1\": int(gt_bboxes[gt_idx][\"x1\"]),\n",
    "                \"y1\": int(gt_bboxes[gt_idx][\"y1\"]),\n",
    "                \"x2\": int(gt_bboxes[gt_idx][\"x2\"]),\n",
    "                \"y2\": int(gt_bboxes[gt_idx][\"y2\"])\n",
    "            } if gt_idx is not None else {},\n",
    "            \"predicted_bbox\": {\n",
    "                \"x1\": int(p[\"x1\"]),\n",
    "                \"y1\": int(p[\"y1\"]),\n",
    "                \"x2\": int(p[\"x2\"]),\n",
    "                \"y2\": int(p[\"y2\"])\n",
    "            },\n",
    "            \"actual\": actual_label,\n",
    "            \"score\": class_id,\n",
    "            \"iou\": best_iou,\n",
    "            \"confidence\": p[\"confidence\"]\n",
    "        })\n",
    "    for i, predicted in enumerate(gt_bboxes_predicted):\n",
    "        if not predicted:\n",
    "            results.append({\n",
    "                \"image\": image_path,\n",
    "                \"actual_bbox\": {\n",
    "                    \"x1\": int(gt_bboxes[i][\"x1\"]),\n",
    "                    \"y1\": int(gt_bboxes[i][\"y1\"]),\n",
    "                    \"x2\": int(gt_bboxes[i][\"x2\"]),\n",
    "                    \"y2\": int(gt_bboxes[i][\"y2\"])\n",
    "                },\n",
    "                \"predicted_bbox\": {},\n",
    "                \"actual\": gt_bboxes[i][\"class\"],\n",
    "                \"score\": 0,\n",
    "                \"iou\": 0,\n",
    "                \"confidence\": 0\n",
    "            })\n",
    "# Save results to JSON\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    import json\n",
    "    json.dump(results, f, indent=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159324b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff93ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93112f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_confidence_threshold = 0.4\n",
    "damaged_confidence_threshold = healthy_confidence_threshold\n",
    "\n",
    "def map_actual(x):\n",
    "    return 'background' if x == 'unknown' else x\n",
    "\n",
    "def map_predicted(score, conf):\n",
    "    healthy_conf = conf * score\n",
    "    damaged_conf = conf * (1 - score)\n",
    "    if healthy_conf >= healthy_confidence_threshold:\n",
    "        if damaged_conf >= damaged_confidence_threshold:\n",
    "            return 'healthy' if healthy_conf >= damaged_conf else 'damaged'\n",
    "        return 'healthy'\n",
    "    elif damaged_conf >= damaged_confidence_threshold:\n",
    "        return 'damaged'\n",
    "    return 'background'\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['actual_mapped'] = df['actual'].apply(map_actual)\n",
    "df['predicted_mapped'] = df.apply(\n",
    "    lambda row: map_predicted(row['score'], row['confidence']), axis=1\n",
    ")\n",
    "\n",
    "classes = ['healthy', 'damaged', 'background']\n",
    "\n",
    "y_true = df['actual_mapped']\n",
    "y_pred = df['predicted_mapped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78b808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "\n",
    "\n",
    "display(cm_df)\n",
    "\n",
    "#cm[2, 2] = 0  # Set background-background to nan\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80672420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Only use 'healthy' and 'damaged' classes\n",
    "labels = ['healthy', 'damaged']\n",
    "precision = precision_score(df['actual_mapped'], df['predicted_mapped'], average=None, labels=labels)\n",
    "recall = recall_score(df['actual_mapped'], df['predicted_mapped'], average=None, labels=labels)\n",
    "f1 = f1_score(df['actual_mapped'], df['predicted_mapped'], average=None, labels=labels)\n",
    "\n",
    "for cls, p, r, f in zip(labels, precision, recall, f1):\n",
    "    print(f\"{cls}: Precision={p:.3f}, Recall={r:.3f}, F1={f:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ious = np.arange(0.5, 1.0, 0.05)\n",
    "aps = []\n",
    "\n",
    "for iou_thr in ious:\n",
    "    # Filter detections by IoU threshold\n",
    "    df_iou = df.copy()\n",
    "\n",
    "    # AP for healthy\n",
    "    y_true_healthy = (df_iou['actual_mapped'] == 'healthy').astype(int)\n",
    "    y_score_healthy = df_iou['confidence'] * df_iou['score'] * (df_iou['iou'] >= iou_thr).astype(float) * (df_iou['score'] >= 0.5).astype(float)\n",
    "    ap_healthy = average_precision_score(y_true_healthy, y_score_healthy) if len(y_true_healthy) > 0 and y_true_healthy.sum() > 0 else np.nan\n",
    "\n",
    "    # AP for damaged\n",
    "    y_true_damaged = (df_iou['actual_mapped'] == 'damaged').astype(int)\n",
    "    y_score_damaged = df_iou['confidence'] * (1 - df_iou['score']) * (df_iou['iou'] >= iou_thr).astype(float) * (df_iou['score'] <= 0.5).astype(float)\n",
    "    ap_damaged = average_precision_score(y_true_damaged, y_score_damaged) if len(y_true_damaged) > 0 and y_true_damaged.sum() > 0 else np.nan\n",
    "\n",
    "    aps.append({'iou': iou_thr, 'AP_healthy': ap_healthy, 'AP_damaged': ap_damaged})\n",
    "\n",
    "# Convert to DataFrame for display\n",
    "aps_df = pd.DataFrame(aps)\n",
    "aps_df['mAP'] = aps_df[['AP_healthy', 'AP_damaged']].mean(axis=1)\n",
    "display(aps_df)\n",
    "map_50_95 = aps_df['mAP'].mean()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(aps_df['iou'], aps_df['AP_healthy'], label='AP Healthy', marker='o', color='green')\n",
    "plt.plot(aps_df['iou'], aps_df['AP_damaged'], label='AP Damaged', marker='o', color='red')\n",
    "plt.plot(aps_df['iou'], aps_df['mAP'], label='mAP', marker='o', color='blue')\n",
    "plt.axhline(y=map_50_95, color='purple', linestyle='--', alpha=0.7, label=f'mAP@0.50:0.95 = {map_50_95:.3f}')\n",
    "plt.xlabel('IoU Threshold')\n",
    "plt.ylabel('Average Precision (AP)')\n",
    "plt.title('AP by class and mAP vs IoU Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"mAP@50:95 = {map_50_95:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "df_det = df.copy()\n",
    "y_true_det = (df_det['actual_mapped'] != 'background').astype(int)\n",
    "y_score_det = df_det['confidence'] \n",
    "\n",
    "precision_det, recall_det, thresholds = precision_recall_curve(y_true_det, y_score_det)\n",
    "ap_det = average_precision_score(y_true_det, y_score_det)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(recall_det, precision_det, label=f\"Detección (AP={ap_det:.2f})\", color='blue')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve (Detección: objeto vs fondo)')\n",
    "plt.legend()\n",
    "#plt.ylim([0.0, 1.05])\n",
    "#plt.xlim([0.0, 1.05])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d76ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df_det = df.copy()\n",
    "\n",
    "# Healthy\n",
    "y_true_healthy = (df_det['actual_mapped'] == 'healthy').astype(int)\n",
    "y_score_healthy = df_det['confidence'] * df_det['score'] * (df_det['score'] >= 0.5).astype(float)\n",
    "precision_healthy, recall_healthy, thresholds_healthy = precision_recall_curve(y_true_healthy, y_score_healthy)\n",
    "ap_healthy = average_precision_score(y_true_healthy, y_score_healthy)\n",
    "\n",
    "# Damaged\n",
    "y_true_damaged = (df_det['actual_mapped'] == 'damaged').astype(int)\n",
    "y_score_damaged = df_det['confidence'] * (1 - df_det['score']) * (df_det['score'] <= 0.5).astype(float)\n",
    "precision_damaged, recall_damaged, thresholds_damaged = precision_recall_curve(y_true_damaged, y_score_damaged)\n",
    "ap_damaged = average_precision_score(y_true_damaged, y_score_damaged)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(recall_healthy, precision_healthy, label=f\"Healthy (AP={ap_healthy:.2f})\", color='green')\n",
    "plt.plot(recall_damaged, precision_damaged, label=f\"Damaged (AP={ap_damaged:.2f})\", color='red')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve by class')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0, 1.01, 0.1))\n",
    "plt.yticks(np.arange(0, 1.01, 0.1))\n",
    "plt.xlim([0.0, 1.02])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(thresholds_healthy, precision_healthy[1:], label=f\"Precision Healthy\", color='limegreen')\n",
    "plt.plot(thresholds_healthy, recall_healthy[1:], label=f\"Recall Healthy\", color='darkgreen')\n",
    "plt.plot(thresholds_damaged, precision_damaged[1:], label=f\"Precision Damaged\", color='red')\n",
    "plt.plot(thresholds_damaged, recall_damaged[1:], label=f\"Recall Damaged\", color='darkred')\n",
    "\n",
    "# Add horizontal dotted lines for max damage recall and 95% of max\n",
    "max_damage_recall = np.max(recall_damaged[1:])\n",
    "recall_95_percent = 0.964 * max_damage_recall\n",
    "plt.axhline(y=max_damage_recall, color='darkred', linestyle='--', alpha=0.7, label=f'Max Damage Recall ({max_damage_recall:.3f})')\n",
    "plt.axhline(y=recall_95_percent, color='darkred', linestyle=':', alpha=0.7, label=f'>95% Max Damage Recall ({recall_95_percent:.3f})')\n",
    "#plt.axvline(x=0.4, color='purple', linestyle='--', alpha=0.7, label='Selected Threshold (0.4)')\n",
    "plt.xlabel('Combined Score Threshold')\n",
    "plt.ylabel('Precision/Recall')\n",
    "plt.title('Precision and Recall vs Combined Score Threshold by class')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0, 1.01, 0.1))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.02])\n",
    "plt.yticks(np.arange(0, 1.01, 0.1))\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compute F1-score for each threshold for healthy\n",
    "f1_healthy = []\n",
    "for thr in thresholds_healthy:\n",
    "    preds = (y_score_healthy >= thr).astype(int)\n",
    "    f1 = f1_score(y_true_healthy, preds)\n",
    "    f1_healthy.append(f1)\n",
    "\n",
    "# Compute F1-score for each threshold for damaged\n",
    "f1_damaged = []\n",
    "for thr in thresholds_damaged:\n",
    "    preds = (y_score_damaged >= thr).astype(int)\n",
    "    f1 = f1_score(y_true_damaged, preds)\n",
    "    f1_damaged.append(f1)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(thresholds_healthy, f1_healthy, label=\"F1 Healthy\", color='green')\n",
    "plt.plot(thresholds_damaged, f1_damaged, label=\"F1 Damaged\", color='red')\n",
    "plt.axvline(x=0.4, color='purple', linestyle='--', alpha=0.7, label='Selected Threshold (0.4)')\n",
    "plt.xlabel('Combined Score Threshold')\n",
    "plt.ylabel('F1-score')\n",
    "plt.title('F1-score vs Combined Score Threshold by class')\n",
    "plt.legend()\n",
    "plt.xticks(np.arange(0, 1.01, 0.1))\n",
    "plt.yticks(np.arange(0, 1.01, 0.1))\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.02])\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c515fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_all_predictions_on_image(img_path, df):\n",
    "    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    sample_df = df[df['image'] == img_path]\n",
    "    img_draw = img.copy()\n",
    "    h_img, w_img = img.shape[:2]\n",
    "    thickness = max(2, int(round(0.005 * (h_img + w_img) / 2)))\n",
    "    font_scale = max(0.5, 0.002 * (h_img + w_img) / 2)\n",
    "    for _, row in sample_df.iterrows():\n",
    "        bbox = row['predicted_bbox'] if row['predicted_bbox'] else row['actual_bbox']\n",
    "        \n",
    "        if bbox and all(k in bbox for k in ['x1', 'y1', 'x2', 'y2']):\n",
    "            x1, y1, x2, y2 = bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']\n",
    "            truth = row['actual_mapped']\n",
    "            label = row['predicted_mapped']\n",
    "\n",
    "            if truth == label:\n",
    "                color = (0, 255, 0) if label != 'background' else (128, 128, 0)\n",
    "            elif label == 'background':\n",
    "                color = (255, 165, 0)\n",
    "            else:\n",
    "                color = (255, 0, 0)\n",
    "            cv2.rectangle(img_draw, (x1, y1), (x2, y2), color, thickness)\n",
    "            cv2.putText(img_draw, label, (x1, y1 + int(font_scale*13)), cv2.FONT_HERSHEY_SIMPLEX, font_scale*0.6, color, int(thickness))\n",
    "    return img_draw\n",
    "\n",
    "# Pick random sample of images\n",
    "N = 20\n",
    "sample_images = random.sample(image_paths, N)\n",
    "for img_path in sample_images:\n",
    "    annotated_img = draw_all_predictions_on_image(img_path, df)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(annotated_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_bbox(img, bbox, label, color, thickness=2, font_scale=0.7):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    img = img.copy()\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n",
    "    cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)\n",
    "    return img\n",
    "\n",
    "def get_bbox_tuple(bbox_dict):\n",
    "    if bbox_dict and all(k in bbox_dict for k in ['x1', 'y1', 'x2', 'y2']):\n",
    "        return (bbox_dict['x1'], bbox_dict['y1'], bbox_dict['x2'], bbox_dict['y2'])\n",
    "    return None\n",
    "\n",
    "def crop_image(img, bbox, pad):\n",
    "    if bbox is None:\n",
    "        return img\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    h, w = img.shape[:2]\n",
    "    x1 = max(0, x1 - pad)\n",
    "    y1 = max(0, y1 - pad)\n",
    "    x2 = min(w, x2 + pad)\n",
    "    y2 = min(h, y2 + pad)\n",
    "    return img[y1:y2, x1:x2]\n",
    "\n",
    "def show_examples_grid(df, N=12, cols=4):\n",
    "    samples = list(df.index)[:N]\n",
    "\n",
    "    rows = int(np.ceil(N / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, idx in enumerate(samples):\n",
    "        row = df.loc[idx]\n",
    "        img_path = row['image']\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        gt_bbox = get_bbox_tuple(row['actual_bbox'])\n",
    "        pred_bbox = get_bbox_tuple(row['predicted_bbox'])\n",
    "\n",
    "        # Crop around the predicted bbox if available, else ground truth bbox, else show full image\n",
    "        crop_bbox = pred_bbox if pred_bbox else gt_bbox\n",
    "        pad = 50\n",
    "        img_crop = crop_image(img, crop_bbox, pad=pad)\n",
    "\n",
    "        img_draw = img_crop.copy()\n",
    "        # Draw ground truth annotation\n",
    "        if gt_bbox:\n",
    "            # Adjust bbox coordinates for crop\n",
    "            if crop_bbox:\n",
    "                x1, y1, x2, y2 = gt_bbox\n",
    "                cx1, cy1, _, _ = crop_bbox\n",
    "                adj_bbox = (max(0, x1-cx1+pad), max(0, y1-cy1+pad), max(0, x2-cx1+pad), max(0, y2-cy1+pad))\n",
    "            else:\n",
    "                adj_bbox = gt_bbox\n",
    "            img_draw = draw_bbox(img_draw, adj_bbox, f\"GT: {row['actual_mapped']}\", color=(0, 255, 0), thickness=3)\n",
    "        # Draw prediction annotation\n",
    "        if pred_bbox:\n",
    "            if crop_bbox:\n",
    "                x1, y1, x2, y2 = pred_bbox\n",
    "                cx1, cy1, _, _ = crop_bbox\n",
    "                adj_bbox = (max(0, x1-cx1+pad), max(0, y1-cy1+pad), max(0, x2-cx1+pad), max(0, y2-cy1+pad))\n",
    "            else:\n",
    "                adj_bbox = pred_bbox\n",
    "            img_draw = draw_bbox(img_draw, adj_bbox, f\"Pred: {row['predicted_mapped']}\", color=(255, 0, 0), thickness=2)\n",
    "\n",
    "        axes[i].imshow(img_draw)\n",
    "        #axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage: show grid of FP, FN, TN examples with annotations\n",
    "print(\"False Positives (FP):\")\n",
    "show_examples_grid(df[(df['actual_mapped'] == 'background') & (df['predicted_mapped'] != 'background')], N=30, cols=5)\n",
    "print(\"False Backgrounds (FN):\")\n",
    "show_examples_grid(df[(df['actual_mapped'] != 'background') & (df['predicted_mapped'] == 'background')], N=75, cols=5)\n",
    "print(\"Misclassifications (FN):\")\n",
    "show_examples_grid(df[(df['actual_mapped'] != 'background') & (df['predicted_mapped'] != 'background') & (df['actual_mapped'] != df['predicted_mapped'])], N=75, cols=5)\n",
    "print(\"True Negatives (TN):\")\n",
    "show_examples_grid(df[(df['actual_mapped'] == 'background') & (df['predicted_mapped'] == 'background')], N=40, cols=5)\n",
    "print(\"True Positives (TP):\")\n",
    "show_examples_grid(df[(df['actual_mapped'] != 'background') & (df['predicted_mapped'] != 'background') & (df['actual_mapped'] == df['predicted_mapped'])], N=75, cols=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
